{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--INFO-- Concatenating raw files in mateo/raw/cited_by/\n",
      "--INFO-- Concatenating raw files in mateo/raw/references/\n",
      "--INFO-- Concatenating raw files in mateo/raw/documents/\n",
      "--INFO-- Applying scopus tags to database files\n",
      "--INFO-- Formating column names in database files\n",
      "--INFO-- Dropping NA columns in database files\n",
      "--INFO-- Removing accents in database files\n",
      "--INFO-- Removing stranger chars in database files\n",
      "--INFO-- Processing `abstract` column\n",
      "--INFO-- Processing `authors_id` column\n",
      "--INFO-- Processing `title` column\n",
      "--INFO-- Processing `document_type` column\n",
      "--INFO-- Processing `doi` column\n",
      "--INFO-- Processing `eissn` column\n",
      "--INFO-- Processing `global_citations` column\n",
      "--INFO-- Processing `isbn` column\n",
      "--INFO-- Processing `issn` column\n",
      "--INFO-- Processing `raw_authors` column\n",
      "--INFO-- Processing `source_abbr` column\n",
      "--INFO-- Processing `source_name` column\n",
      "--INFO-- Processing `global_references` column\n",
      "--INFO-- Creating `authors` column\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/GitHub/techminer2/data/_debug.ipynb Celda 1\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/GitHub/techminer2/data/_debug.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/GitHub/techminer2/data/_debug.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m directory \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmateo\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/GitHub/techminer2/data/_debug.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m tm2__import_scopus_files(directory)\n",
      "File \u001b[0;32m/Volumes/GitHub/techminer2/techminer2/tm2__import_scopus_files.py:123\u001b[0m, in \u001b[0;36mtm2__import_scopus_files\u001b[0;34m(directory, disable_progress_bar, **document_types)\u001b[0m\n\u001b[1;32m    121\u001b[0m _process__global_references__column(directory)\n\u001b[1;32m    122\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m _create__authors__column(directory)\n\u001b[1;32m    124\u001b[0m _create__num_authors__column(directory)\n\u001b[1;32m    125\u001b[0m _create__article__column(directory)\n",
      "File \u001b[0;32m/Volumes/GitHub/techminer2/techminer2/tm2__import_scopus_files.py:1325\u001b[0m, in \u001b[0;36m_create__authors__column\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m   1323\u001b[0m data \u001b[39m=\u001b[39m load_authors_names()\n\u001b[1;32m   1324\u001b[0m author_id2name \u001b[39m=\u001b[39m build_dict_names(data)\n\u001b[0;32m-> 1325\u001b[0m repair_names(author_id2name)\n",
      "File \u001b[0;32m/Volumes/GitHub/techminer2/techminer2/tm2__import_scopus_files.py:1317\u001b[0m, in \u001b[0;36m_create__authors__column.<locals>.repair_names\u001b[0;34m(author_id2name)\u001b[0m\n\u001b[1;32m   1315\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39massign(authors\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mauthors\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m; \u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1316\u001b[0m \u001b[39mfor\u001b[39;00m author_id, author \u001b[39min\u001b[39;00m author_id2name\u001b[39m.\u001b[39mitems():\n\u001b[0;32m-> 1317\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39massign(authors\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39;49mauthors\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39mreplace(author_id, author))\n\u001b[1;32m   1318\u001b[0m data\u001b[39m.\u001b[39mto_csv(file, sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Volumes/GitHub/techminer2/.venv/lib/python3.8/site-packages/pandas/core/accessor.py:181\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessor\n\u001b[0;32m--> 181\u001b[0m accessor_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor(obj)\n\u001b[1;32m    182\u001b[0m \u001b[39m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m# NDFrame\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m/Volumes/GitHub/techminer2/.venv/lib/python3.8/site-packages/pandas/core/strings/accessor.py:168\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m    166\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstring_\u001b[39;00m \u001b[39mimport\u001b[39;00m StringDtype\n\u001b[0;32m--> 168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(data)\n\u001b[1;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_categorical \u001b[39m=\u001b[39m is_categorical_dtype(data\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_string \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(data\u001b[39m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[0;32m/Volumes/GitHub/techminer2/.venv/lib/python3.8/site-packages/pandas/core/strings/accessor.py:222\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    218\u001b[0m data \u001b[39m=\u001b[39m _extract_array(data)\n\u001b[1;32m    220\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mcategories\u001b[39m\u001b[39m\"\u001b[39m, data)  \u001b[39m# categorical / normal\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m inferred_dtype \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49minfer_dtype(values, skipna\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    224\u001b[0m \u001b[39mif\u001b[39;00m inferred_dtype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_types:\n\u001b[1;32m    225\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan only use .str accessor with string values!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "currentdir = os.getcwd()\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.append(parentdir)\n",
    "\n",
    "# from itables import init_notebook_mode\n",
    "# init_notebook_mode(all_interactive=True)\n",
    "# --- \n",
    "from techminer2 import *\n",
    "import pandas as pd\n",
    "directory = \"mateo\"\n",
    "\n",
    "tm2__import_scopus_files(directory)\n",
    "# import_references()\n",
    "# annual_scientific_production()\n",
    "# main_information(directory)\n",
    "\n",
    "# with open(os.path.join(directory, \"processed/_documents.csv\"), \"r\") as f:\n",
    "#     documents = pd.read_csv(f)\n",
    "#     abstracts = documents[\"abstract\"]\n",
    "#     abstracts = abstracts.dropna()\n",
    "#     abstracts = abstracts.str.split(\".\")\n",
    "#     abstracts = abstracts.map(lambda x: [y for y in x if y != \"\"])\n",
    "#     abstracts = abstracts.str[-1]\n",
    "# abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "directory = \"regtech\"\n",
    "pd.read_csv(f\"{directory}/processed/_documents.csv\").local_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "\n",
    "def _extract_keywords_from_database_files(directory):\n",
    "    keywords_list = []\n",
    "    files = list(glob.glob(os.path.join(directory, \"processed/_*.csv\")))\n",
    "    for file in files:\n",
    "        data = pd.read_csv(file, encoding=\"utf-8\")\n",
    "        if \"raw_index_keywords\" in data.columns:\n",
    "            keywords_list += data.raw_index_keywords.dropna().tolist()\n",
    "        if \"raw_author_keywords\" in data.columns:\n",
    "            keywords_list += data.raw_author_keywords.dropna().tolist()\n",
    "    keywords_list = pd.DataFrame({\"keyword\": keywords_list})\n",
    "    keywords_list = keywords_list.assign(keyword=keywords_list.keyword.str.split(\";\"))\n",
    "    keywords_list = keywords_list.explode(\"keyword\")\n",
    "    keywords_list = keywords_list.keyword.str.strip()\n",
    "    keywords_list = keywords_list.drop_duplicates()\n",
    "    keywords_list = keywords_list.reset_index(drop=True)\n",
    "    return keywords_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "def _extract_keywords_roots_from_database_files(directory):\n",
    "    \n",
    "    keywords_list = []\n",
    "    \n",
    "    files = list(glob.glob(os.path.join(directory, \"processed/_*.csv\")))\n",
    "    for file in files:\n",
    "        data = pd.read_csv(file, encoding=\"utf-8\")\n",
    "        for column in [\"raw_author_keywords\", \"raw_index_keywords\"]:\n",
    "            keywords_list.append(data[column])\n",
    "\n",
    "    keywords_list = pd.concat(keywords_list)\n",
    "    keywords_list = keywords_list.dropna()\n",
    "    keywords_list = keywords_list.str.split(\";\")\n",
    "    keywords_list = keywords_list.explode()\n",
    "    keywords_list = keywords_list.str.strip()\n",
    "    keywords_list = keywords_list.drop_duplicates()\n",
    "\n",
    "    keywords_list = keywords_list.str.replace(r\"\\[.+\\]\", \"\", regex=True)\n",
    "    keywords_list = keywords_list.str.replace(r\"\\(.+\\)\", \"\", regex=True)\n",
    "    keywords_list = keywords_list.str.replace(r\"-\", \" \", regex=False)\n",
    "    keywords_list = keywords_list.str.replace(r\"&\", \" \", regex=False)\n",
    "\n",
    "    # list of single words\n",
    "    keywords_list = keywords_list.str.split()\n",
    "    s = PorterStemmer()\n",
    "    keywords_list = keywords_list.map(lambda x: [s.stem(y) for y in x])\n",
    "    keywords_list = keywords_list.map(set)\n",
    "    keywords_list = keywords_list.map(sorted)\n",
    "    keywords_list = keywords_list.map(lambda x: \" \".join(x))\n",
    "\n",
    "    return keywords_list\n",
    "\n",
    "\n",
    "def _create__raw_words__column(directory, source_column, dest_column):\n",
    "\n",
    "    #\n",
    "    roots = _extract_keywords_roots_from_database_files(directory).to_list()\n",
    "    \n",
    "    #\n",
    "    files = list(glob.glob(os.path.join(directory, \"processed/_*.csv\")))\n",
    "    for file in files:\n",
    "        data = pd.read_csv(file, encoding=\"utf-8\")\n",
    "        #\n",
    "        if source_column not in data.columns:\n",
    "            continue\n",
    "\n",
    "        sys.stdout.write(f\"--INFO-- Creating `{dest_column}` column in {file}\\n\")\n",
    "\n",
    "        text = data[['article', source_column]].copy()\n",
    "        text = text.dropna()\n",
    "        \n",
    "        text[source_column] = text[source_column].str.lower().copy()\n",
    "        \n",
    "        \n",
    "        text['raw_words'] = text[source_column].map(lambda x: TextBlob(x).noun_phrases)\n",
    "        text = text.explode('raw_words')\n",
    "        text = text.dropna()\n",
    "        text['keys'] = text['raw_words'].str.split()\n",
    "        s = PorterStemmer()\n",
    "        text['keys'] = text['keys'].map(lambda x: [s.stem(y) for y in x])\n",
    "        text['keys'] = text['keys'].map(set)\n",
    "        text['keys'] = text['keys'].map(sorted)\n",
    "        text['keys'] = text['keys'].map(lambda x: \" \".join(x))\n",
    "        \n",
    "        text['found'] = text['keys'].map(lambda x: x in roots)\n",
    "        text = text[text['found'] == True]\n",
    "\n",
    "        text = text[['article', 'raw_words']]\n",
    "        text = text.groupby('article', as_index=False).aggregate(lambda x: list(x))\n",
    "        text['raw_words'] = text['raw_words'].map(set)\n",
    "        text['raw_words'] = text['raw_words'].map(sorted)\n",
    "        text['raw_words'] = text['raw_words'].str.join(\"; \")\n",
    "\n",
    "        # convert the pandas series to a dictionary\n",
    "        values_dict = dict(zip(text.article, text.raw_words))\n",
    "        data[dest_column] = data['article'].map(lambda x: values_dict.get(x, pd.NA))\n",
    "        #\n",
    "        data.to_csv(file, sep=\",\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "\n",
    "directory = \"regtech\"\n",
    "_create__raw_abstract_words__column(directory, source_column='abstract', dest_column='raw_abstract_words')\n",
    "_create__raw_abstract_words__column(directory, source_column='title', dest_column='raw_title_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "directory = \"regtech\"\n",
    "\n",
    "documents = pd.read_csv(f\"{directory}/processed/_documents.csv\")\n",
    "\n",
    "index=2\n",
    "print(documents.raw_abstract_words[index])\n",
    "print(documents.raw_title_words[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents.raw_author_keywords[index])\n",
    "print(documents.raw_index_keywords[index])\n",
    "print(documents.title[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents.raw_index_keywords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff8a4715c70c0e4b3cc8412ea892d5df3eeb381ac1fbe083ff6ed76fad7df612"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
